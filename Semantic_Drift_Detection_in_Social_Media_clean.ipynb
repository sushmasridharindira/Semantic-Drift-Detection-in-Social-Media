{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sushmasridharindira/Semantic-Drift-Detection-in-Social-Media/blob/main/Semantic_Drift_Detection_in_Social_Media_clean.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install praw"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6TexBfRHSxN",
        "outputId": "feb20441-a128-4042-aaa4-6ca47b61f3b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting praw\n",
            "  Downloading praw-7.8.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Collecting prawcore<3,>=2.4 (from praw)\n",
            "  Downloading prawcore-2.4.0-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting update_checker>=0.18 (from praw)\n",
            "  Downloading update_checker-0.18.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.12/dist-packages (from praw) (1.9.0)\n",
            "Requirement already satisfied: requests<3.0,>=2.6.0 in /usr/local/lib/python3.12/dist-packages (from prawcore<3,>=2.4->praw) (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2025.11.12)\n",
            "Downloading praw-7.8.1-py3-none-any.whl (189 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m189.3/189.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading prawcore-2.4.0-py3-none-any.whl (17 kB)\n",
            "Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
            "Installing collected packages: update_checker, prawcore, praw\n",
            "Successfully installed praw-7.8.1 prawcore-2.4.0 update_checker-0.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zBYtQKO1fnmU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Scrape Reddit posts for multiple keywords using PRAW.\n",
        "Saves a combined CSV with columns: id, timestamp, text\n",
        "\"\"\"\n",
        "\n",
        "import praw\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "# ðŸ”‘ Fill in your Reddit app credentials (script type)\n",
        "CLIENT_ID = \"kFNK7dZXGykyxkMn3YdI4g\"\n",
        "CLIENT_SECRET = \"VmIj9F9n1qI_LYxIj0yrHysA3_VbMw\"\n",
        "USER_AGENT = \"semantic-drift-research by u/Bubbly-Active-6675\"\n",
        "\n",
        "# Initialize Reddit API client\n",
        "reddit = praw.Reddit(\n",
        "    client_id=CLIENT_ID,\n",
        "    client_secret=CLIENT_SECRET,\n",
        "    user_agent=USER_AGENT,\n",
        ")\n",
        "\n",
        "def scrape_keyword(keyword, subreddit=\"all\", limit=500):\n",
        "    \"\"\"\n",
        "    Scrape posts for a single keyword.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame with id, timestamp, text\n",
        "    \"\"\"\n",
        "    posts = []\n",
        "    for submission in reddit.subreddit(subreddit).search(keyword, sort=\"new\", limit=limit):\n",
        "        posts.append({\n",
        "            \"id\": submission.id,\n",
        "            \"timestamp\": datetime.utcfromtimestamp(submission.created_utc),\n",
        "            \"text\": submission.title + \" \" + (submission.selftext or \"\")\n",
        "        })\n",
        "    return pd.DataFrame(posts)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    keywords = [\"woke\", \"cloud\", \"nurse\"]\n",
        "    all_dfs = []\n",
        "\n",
        "    for kw in keywords:\n",
        "        print(f\"Scraping posts for keyword: {kw}\")\n",
        "        df = scrape_keyword(kw, subreddit=\"all\", limit=500)\n",
        "        all_dfs.append(df)\n",
        "\n",
        "    # Combine all keywords into one CSV\n",
        "    combined_df = pd.concat(all_dfs, ignore_index=True).drop_duplicates(subset=[\"id\"])\n",
        "    combined_df.to_csv(\"reddit_combined.csv\", index=False)\n",
        "    print(f\"âœ… Saved {len(combined_df)} posts â†’ reddit_combined.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J9XWRSAKHcSU",
        "outputId": "50f11686-6dd5-4714-d7f0-7e5735580798"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping posts for keyword: woke\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3974428366.py:34: DeprecationWarning: datetime.datetime.utcfromtimestamp() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.fromtimestamp(timestamp, datetime.UTC).\n",
            "  \"timestamp\": datetime.utcfromtimestamp(submission.created_utc),\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping posts for keyword: cloud\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping posts for keyword: nurse\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Saved 736 posts â†’ reddit_combined.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Optimizations for Reddit:\n",
        "- Clean markdown quotes, code blocks, and links\n",
        "- Increased tokenizer max_length=512\n",
        "- Works for posts and comments of variable lengths\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "import argparse\n",
        "import os\n",
        "import json\n",
        "from dataclasses import dataclass, field\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from sklearn.metrics import pairwise_distances\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# -----------------------------\n",
        "# Utility & Config\n",
        "# -----------------------------\n",
        "@dataclass\n",
        "class Config:\n",
        "    data_path: str = \"reddit_combined.csv\"\n",
        "    out_dir: str = \"outputs_reddit\"\n",
        "    targets: List[str] = field(default_factory=lambda: [\"woke\", \"cloud\", \"nurse\"])\n",
        "    model_name: str = \"bert-base-uncased\"\n",
        "    max_docs: Optional[int] = None\n",
        "    time_split: Optional[str] = None\n",
        "    window_days: Optional[int] = None\n",
        "    stride_days: Optional[int] = None\n",
        "    min_occurrences_per_slice: int = 30\n",
        "    clustering: str = \"kmeans\"\n",
        "    kmeans_k: Optional[int] = None\n",
        "    dbscan_eps: float = 0.8\n",
        "    dbscan_min_samples: int = 10\n",
        "    random_state: int = 42\n",
        "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Preprocessing for Reddit\n",
        "# -----------------------------\n",
        "def clean_reddit_text(text: str) -> str:\n",
        "    \"\"\"Remove markdown quotes, code blocks, and links from Reddit posts/comments.\"\"\"\n",
        "    text = re.sub(r\"^>.*$\", \"\", text, flags=re.MULTILINE)\n",
        "    text = re.sub(r\"```.*?```\", \"\", text, flags=re.DOTALL)\n",
        "    text = re.sub(r\"http\\S+|www.\\S+\", \"\", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Data Loading & Slicing\n",
        "# -----------------------------\n",
        "def load_corpus(path: str, max_docs: Optional[int] = None) -> pd.DataFrame:\n",
        "    ext = os.path.splitext(path)[1].lower()\n",
        "    if ext == \".csv\":\n",
        "        df = pd.read_csv(path)\n",
        "    elif ext in {\".jsonl\", \".json\"}:\n",
        "        df = pd.read_json(path, lines=True)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file extension. Use .csv or .jsonl\")\n",
        "\n",
        "    if \"timestamp\" not in df.columns or \"text\" not in df.columns:\n",
        "        raise ValueError(\"Data must contain 'timestamp' and 'text' columns.\")\n",
        "\n",
        "    df = df.dropna(subset=[\"text\", \"timestamp\"]).copy()\n",
        "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], errors=\"coerce\")\n",
        "    df = df.dropna(subset=[\"timestamp\"])\n",
        "    df = df.sort_values(\"timestamp\")\n",
        "\n",
        "    if max_docs is not None:\n",
        "        df = df.head(max_docs)\n",
        "\n",
        "    # Clean text for Reddit\n",
        "    df[\"text\"] = df[\"text\"].astype(str).apply(clean_reddit_text)\n",
        "    return df\n",
        "\n",
        "\n",
        "def build_time_slices(\n",
        "    df: pd.DataFrame,\n",
        "    time_split: Optional[str] = None,\n",
        "    window_days: Optional[int] = None,\n",
        "    stride_days: Optional[int] = None,\n",
        ") -> List[Tuple[str, pd.DatetimeIndex]]:\n",
        "\n",
        "    if time_split:\n",
        "        split = pd.to_datetime(time_split)\n",
        "        mask1 = df[\"timestamp\"] < split\n",
        "        mask2 = df[\"timestamp\"] >= split\n",
        "        return [\n",
        "            (f\"pre_{split.date()}\", df.index[mask1]),\n",
        "            (f\"post_{split.date()}\", df.index[mask2]),\n",
        "        ]\n",
        "\n",
        "    elif window_days and stride_days:\n",
        "        start = df[\"timestamp\"].min().normalize()\n",
        "        end = df[\"timestamp\"].max().normalize()\n",
        "\n",
        "        slices = []\n",
        "        cur = start\n",
        "        delta = pd.Timedelta(days=window_days)\n",
        "        stride = pd.Timedelta(days=stride_days)\n",
        "\n",
        "        while cur <= end:\n",
        "            win_start = cur\n",
        "            win_end = cur + delta\n",
        "            mask = (df[\"timestamp\"] >= win_start) & (df[\"timestamp\"] < win_end)\n",
        "            slices.append((f\"{win_start.date()}_{win_end.date()}\", df.index[mask]))\n",
        "            cur += stride\n",
        "\n",
        "        return slices\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"Provide either --time_split OR (--window_days and --stride_days).\")\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Target Matching & Embeddings\n",
        "# -----------------------------\n",
        "def find_token_spans(text: str, target: str) -> List[Tuple[int, int]]:\n",
        "    spans = []\n",
        "    t_low = target.lower()\n",
        "    low = text.lower()\n",
        "    start = 0\n",
        "\n",
        "    while True:\n",
        "        i = low.find(t_low, start)\n",
        "        if i == -1:\n",
        "            break\n",
        "\n",
        "        left_ok = i == 0 or not low[i - 1].isalnum()\n",
        "        right_ok = i + len(t_low) == len(low) or not low[i + len(t_low)].isalnum()\n",
        "\n",
        "        if left_ok and right_ok:\n",
        "            spans.append((i, i + len(t_low)))\n",
        "\n",
        "        start = i + len(t_low)\n",
        "\n",
        "    return spans\n",
        "\n",
        "\n",
        "def extract_token_embeddings(\n",
        "    texts: List[str],\n",
        "    target: str,\n",
        "    tokenizer: AutoTokenizer,\n",
        "    model: AutoModel,\n",
        "    device: str = \"cpu\",\n",
        ") -> np.ndarray:\n",
        "\n",
        "    model.eval()\n",
        "    all_embeds = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for text in texts:\n",
        "            spans = find_token_spans(text, target)\n",
        "            if not spans:\n",
        "                continue\n",
        "\n",
        "            enc = tokenizer(\n",
        "                text,\n",
        "                return_tensors=\"pt\",\n",
        "                return_offsets_mapping=True,\n",
        "                truncation=True,\n",
        "                padding=False,\n",
        "                max_length=512,\n",
        "            )\n",
        "            enc = {k: v.to(device) if hasattr(v, \"to\") else v for k, v in enc.items()}\n",
        "            outputs = model(input_ids=enc[\"input_ids\"], attention_mask=enc[\"attention_mask\"])\n",
        "\n",
        "            hidden = outputs.last_hidden_state.squeeze(0)\n",
        "            offsets = enc[\"offset_mapping\"].squeeze(0).cpu().numpy()\n",
        "\n",
        "            for (s, e) in spans:\n",
        "                token_idxs = [\n",
        "                    ti for ti, (a, b) in enumerate(offsets)\n",
        "                    if not (a == b) and not (b <= s or a >= e)\n",
        "                ]\n",
        "                if not token_idxs:\n",
        "                    continue\n",
        "\n",
        "                vec = hidden[token_idxs].mean(dim=0)\n",
        "                all_embeds.append(vec.cpu().numpy())\n",
        "\n",
        "    if not all_embeds:\n",
        "        return np.empty((0, model.config.hidden_size))\n",
        "\n",
        "    return np.vstack(all_embeds)\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Clustering & Metrics\n",
        "# -----------------------------\n",
        "def auto_kmeans_k(X: np.ndarray, k_min: int = 2, k_max: int = 5, random_state: int = 42) -> int:\n",
        "    best_k, best_score = k_min, float(\"inf\")\n",
        "\n",
        "    for k in range(k_min, k_max + 1):\n",
        "        km = KMeans(n_clusters=k, n_init=10, random_state=random_state)\n",
        "        labels = km.fit_predict(X)\n",
        "\n",
        "        score = 0.0\n",
        "        for c in range(k):\n",
        "            cluster_pts = X[labels == c]\n",
        "            if len(cluster_pts) <= 1:\n",
        "                continue\n",
        "\n",
        "            dists = pairwise_distances(cluster_pts, [km.cluster_centers_[c]])\n",
        "            score += dists.mean()\n",
        "\n",
        "        score /= k\n",
        "\n",
        "        if score < best_score:\n",
        "            best_k, best_score = k, score\n",
        "\n",
        "    return best_k\n",
        "\n",
        "\n",
        "def cluster_embeddings(X: np.ndarray, cfg: Config) -> Tuple[np.ndarray, Dict]:\n",
        "    if cfg.clustering == \"kmeans\":\n",
        "        k = cfg.kmeans_k or auto_kmeans_k(X, 2, 5, cfg.random_state)\n",
        "        km = KMeans(n_clusters=k, n_init=10, random_state=cfg.random_state)\n",
        "        labels = km.fit_predict(X)\n",
        "        info = {\"centers\": km.cluster_centers_.tolist(), \"k\": k}\n",
        "\n",
        "    elif cfg.clustering == \"dbscan\":\n",
        "        db = DBSCAN(eps=cfg.dbscan_eps, min_samples=cfg.dbscan_min_samples)\n",
        "        labels = db.fit_predict(X)\n",
        "        centers = [X[labels == c].mean(axis=0) for c in set(labels) if c != -1]\n",
        "        info = {\"centers\": [c.tolist() for c in centers], \"k\": len(centers)}\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"Unknown clustering method\")\n",
        "\n",
        "    return labels, info\n",
        "\n",
        "\n",
        "def centroid_distance(c1: np.ndarray, c2: np.ndarray) -> float:\n",
        "    return float(np.linalg.norm(c1 - c2))\n",
        "\n",
        "\n",
        "def js_divergence(p: np.ndarray, q: np.ndarray, eps: float = 1e-9) -> float:\n",
        "    p = p.astype(float)\n",
        "    q = q.astype(float)\n",
        "    p = p / (p.sum() + eps)\n",
        "    q = q / (q.sum() + eps)\n",
        "    m = 0.5 * (p + q)\n",
        "\n",
        "    def kl(a, b):\n",
        "        a = np.clip(a, eps, 1)\n",
        "        b = np.clip(b, eps, 1)\n",
        "        return float((a * np.log(a / b)).sum())\n",
        "\n",
        "    return 0.5 * kl(p, m) + 0.5 * kl(q, m)\n",
        "\n",
        "\n",
        "def entropy(p: np.ndarray, eps: float = 1e-9) -> float:\n",
        "    p = p.astype(float)\n",
        "    p = p / (p.sum() + eps)\n",
        "    p = np.clip(p, eps, 1)\n",
        "    return float(-(p * np.log(p)).sum())\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Visualization & Timeline\n",
        "# -----------------------------\n",
        "def save_pca_scatter(X: np.ndarray, labels: np.ndarray, out_path: str, title: str = \"PCA\"):\n",
        "    if X.shape[0] == 0:\n",
        "        return\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    Xs = scaler.fit_transform(X)\n",
        "\n",
        "    pca = PCA(n_components=2, random_state=42)\n",
        "    X2 = pca.fit_transform(Xs)\n",
        "\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    plt.scatter(X2[:, 0], X2[:, 1], s=6, alpha=0.7, c=labels)\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"PC1\")\n",
        "    plt.ylabel(\"PC2\")\n",
        "    plt.tight_layout()\n",
        "\n",
        "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
        "    plt.savefig(out_path, dpi=180)\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def save_timeline(values: List[Tuple[pd.Timestamp, float]], out_path: str, title: str):\n",
        "    if not values:\n",
        "        return\n",
        "\n",
        "    dates = [d for d, _ in values]\n",
        "    vals = np.array([v for _, v in values], dtype=float)\n",
        "\n",
        "    if len(vals) >= 5:\n",
        "        mu, sigma = vals.mean(), vals.std() + 1e-9\n",
        "        z = (vals - mu) / sigma\n",
        "    else:\n",
        "        z = vals\n",
        "\n",
        "    plt.figure(figsize=(7, 3))\n",
        "    plt.plot(dates, z, marker=\"o\")\n",
        "    plt.title(title + \" (z-scored)\")\n",
        "    plt.xlabel(\"Time slice end\")\n",
        "    plt.ylabel(\"Drift signal (z)\")\n",
        "    plt.tight_layout()\n",
        "\n",
        "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
        "    plt.savefig(out_path, dpi=180)\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Orchestration\n",
        "# -----------------------------\n",
        "def run_pipeline(cfg: Config):\n",
        "    os.makedirs(cfg.out_dir, exist_ok=True)\n",
        "\n",
        "    fig_dir = os.path.join(cfg.out_dir, \"figures\")\n",
        "    emb_dir = os.path.join(cfg.out_dir, \"embeddings\")\n",
        "    met_dir = os.path.join(cfg.out_dir, \"metrics\")\n",
        "    for d in [fig_dir, emb_dir, met_dir]:\n",
        "        os.makedirs(d, exist_ok=True)\n",
        "\n",
        "    print(\"Loading corpusâ€¦\")\n",
        "    df = load_corpus(cfg.data_path, cfg.max_docs)\n",
        "\n",
        "    slices = build_time_slices(df, cfg.time_split, cfg.window_days, cfg.stride_days)\n",
        "\n",
        "    print(f\"Loading model: {cfg.model_name}\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(cfg.model_name)\n",
        "    model = AutoModel.from_pretrained(cfg.model_name).to(cfg.device)\n",
        "\n",
        "    for target in cfg.targets:\n",
        "        print(f\"\\n=== Target: {target} ===\")\n",
        "        slice_results = []\n",
        "\n",
        "        for slice_name, idx in slices:\n",
        "            texts = df.loc[idx, \"text\"].tolist()\n",
        "            embeds = extract_token_embeddings(texts, target, tokenizer, model, cfg.device)\n",
        "\n",
        "            print(f\"Slice {slice_name}: {embeds.shape[0]} occurrences\")\n",
        "\n",
        "            if embeds.shape[0] < cfg.min_occurrences_per_slice:\n",
        "                print(\" -> Skipping (too few occurrences).\")\n",
        "                slice_results.append((slice_name, embeds, None, {\"k\": 0, \"centers\": []}))\n",
        "                continue\n",
        "\n",
        "            labels, info = cluster_embeddings(embeds, cfg)\n",
        "\n",
        "            np.save(os.path.join(emb_dir, f\"{target}_{slice_name}.npy\"), embeds)\n",
        "            save_pca_scatter(\n",
        "                embeds,\n",
        "                labels,\n",
        "                os.path.join(fig_dir, f\"{target}_pca_{slice_name}.png\"),\n",
        "                title=f\"{target} â€“ {slice_name} ({cfg.clustering})\",\n",
        "            )\n",
        "\n",
        "            slice_results.append((slice_name, embeds, labels, info))\n",
        "\n",
        "        # ----- Compute Metrics -----\n",
        "        metrics: Dict[str, Dict] = {}\n",
        "\n",
        "        # centroid distance\n",
        "        consecutive_centroids = []\n",
        "        for slice_name, embeds, labels, info in slice_results:\n",
        "            c = np.array(embeds).mean(axis=0) if embeds.size else None\n",
        "            consecutive_centroids.append((slice_name, c))\n",
        "\n",
        "        centroid_dists = []\n",
        "        for i in range(1, len(consecutive_centroids)):\n",
        "            prev_name, c1 = consecutive_centroids[i - 1]\n",
        "            cur_name, c2 = consecutive_centroids[i]\n",
        "            dist = centroid_distance(c1, c2) if c1 is not None and c2 is not None else None\n",
        "            centroid_dists.append(((prev_name, cur_name), dist))\n",
        "\n",
        "        # JS divergence + entropy delta\n",
        "        js_list = []\n",
        "        ent_list = []\n",
        "\n",
        "        for i in range(1, len(slice_results)):\n",
        "            (name1, _E1, lab1, info1) = slice_results[i - 1]\n",
        "            (name2, _E2, lab2, info2) = slice_results[i]\n",
        "\n",
        "            js_val = None\n",
        "            ent_delta = None\n",
        "\n",
        "            if (\n",
        "                lab1 is not None and lab2 is not None\n",
        "                and info1.get(\"k\", 0) > 0\n",
        "                and info2.get(\"k\", 0) > 0\n",
        "            ):\n",
        "                k = max(info1[\"k\"], info2[\"k\"])\n",
        "\n",
        "                def hist(lab, k):\n",
        "                    valid = lab[lab >= 0] if isinstance(lab, np.ndarray) else np.array(\n",
        "                        [x for x in lab if x >= 0]\n",
        "                    )\n",
        "                    h = np.zeros(k)\n",
        "                    if valid.size:\n",
        "                        counts = np.bincount(valid, minlength=k)\n",
        "                        h[: len(counts)] = counts\n",
        "                    return h\n",
        "\n",
        "                h1, h2 = hist(lab1, k), hist(lab2, k)\n",
        "\n",
        "                js_val = js_divergence(h1, h2)\n",
        "                ent_delta = abs(entropy(h2) - entropy(h1))\n",
        "\n",
        "            js_list.append(((name1, name2), js_val))\n",
        "            ent_list.append(((name1, name2), ent_delta))\n",
        "\n",
        "        metrics[\"centroid_distance_consecutive\"] = [\n",
        "            {\"from\": a, \"to\": b, \"value\": (None if v is None else float(v))}\n",
        "            for (a, b), v in centroid_dists\n",
        "        ]\n",
        "\n",
        "        metrics[\"js_divergence_consecutive\"] = [\n",
        "            {\"from\": a, \"to\": b, \"value\": (None if v is None else float(v))}\n",
        "            for (a, b), v in js_list\n",
        "        ]\n",
        "\n",
        "        metrics[\"entropy_delta_consecutive\"] = [\n",
        "            {\"from\": a, \"to\": b, \"value\": (None if v is None else float(v))}\n",
        "            for (a, b), v in ent_list\n",
        "        ]\n",
        "\n",
        "        # ----- Timeline drift (if sliding windows) -----\n",
        "        if cfg.window_days and cfg.stride_days:\n",
        "            timeline_vals: List[Tuple[pd.Timestamp, float]] = []\n",
        "\n",
        "            for ((a, b), v), (slice_name, idx) in zip(centroid_dists, slices[1:]):\n",
        "                end_date = df.loc[idx, \"timestamp\"].max() if len(idx) else pd.NaT\n",
        "                if pd.isna(end_date) or v is None:\n",
        "                    continue\n",
        "\n",
        "                timeline_vals.append((end_date, float(v)))\n",
        "\n",
        "            save_timeline(\n",
        "                timeline_vals,\n",
        "                os.path.join(fig_dir, f\"{target}_timeline.png\"),\n",
        "                title=f\"Early drift for '{target}'\",\n",
        "            )\n",
        "\n",
        "            metrics[\"early_drift_points\"] = [\n",
        "                {\"date\": str(d.date()), \"signal\": float(v)}\n",
        "                for d, v in timeline_vals\n",
        "            ]\n",
        "\n",
        "        with open(os.path.join(met_dir, f\"{target}_metrics.json\"), \"w\") as f:\n",
        "            json.dump(metrics, f, indent=2)\n",
        "\n",
        "        print(f\"Saved metrics for {target} -> {os.path.join(met_dir, f'{target}_metrics.json')}\")\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# CLI\n",
        "# -----------------------------\n",
        "def parse_args() -> Config:\n",
        "    p = argparse.ArgumentParser(description=\"Semantic Drift Detection Pipeline for Reddit\")\n",
        "    p.add_argument(\"--data_path\", required=True)\n",
        "    p.add_argument(\"--out_dir\", required=True)\n",
        "    p.add_argument(\"--targets\", nargs=\"+\", required=True)\n",
        "    p.add_argument(\"--model\", default=\"bert-base-uncased\")\n",
        "    p.add_argument(\"--max_docs\", type=int, default=None)\n",
        "    p.add_argument(\"--time_split\", type=str, default=None)\n",
        "    p.add_argument(\"--window_days\", type=int, default=None)\n",
        "    p.add_argument(\"--stride_days\", type=int, default=None)\n",
        "    p.add_argument(\"--min_occurrences_per_slice\", type=int, default=30)\n",
        "    p.add_argument(\"--clustering\", choices=[\"kmeans\", \"dbscan\"], default=\"kmeans\")\n",
        "    p.add_argument(\"--kmeans_k\", type=int, default=None)\n",
        "    p.add_argument(\"--dbscan_eps\", type=float, default=0.8)\n",
        "    p.add_argument(\"--dbscan_min_samples\", type=int, default=10)\n",
        "    p.add_argument(\"--seed\", type=int, default=42)\n",
        "\n",
        "    args = p.parse_args()\n",
        "\n",
        "    return Config(\n",
        "        data_path=args.data_path,\n",
        "        out_dir=args.out_dir,\n",
        "        targets=args.targets,\n",
        "        model_name=args.model,\n",
        "        max_docs=args.max_docs,\n",
        "        time_split=args.time_split,\n",
        "        window_days=args.window_days,\n",
        "        stride_days=args.stride_days,\n",
        "        min_occurrences_per_slice=args.min_occurrences_per_slice,\n",
        "        clustering=args.clustering,\n",
        "        kmeans_k=args.kmeans_k,\n",
        "        dbscan_eps=args.dbscan_eps,\n",
        "        dbscan_min_samples=args.dbscan_min_samples,\n",
        "        random_state=args.seed,\n",
        "    )\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Main\n",
        "# -----------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    config = Config(\n",
        "        data_path=\"reddit_combined.csv\",\n",
        "        out_dir=\"outputs_reddit\",\n",
        "        targets=[\"woke\", \"cloud\", \"nurse\"],\n",
        "        window_days=1,\n",
        "        stride_days=1,\n",
        "        min_occurrences_per_slice=1,\n",
        "    )\n",
        "    run_pipeline(config)"
      ],
      "metadata": {
        "id": "GX2RTFt0foOM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537,
          "referenced_widgets": [
            "9b528a13df1a47d5a7b88d190d49d786",
            "40251efd21a04eb082ed92ff2a070382",
            "c57b9e1e3d7e4e6a86336e691112840d",
            "f9526f0aeb3542cb8777663aaa18959b",
            "7bfed6738ed94a11bb8ed8260fa3d9b7",
            "c6565c5885fd45019d7060b39d8df677",
            "d73c0b0c075a49008c4911d347ec8211",
            "4d19b3a9a04140f08d54ce554b9f8627",
            "b4f9ca30ced444a68a74b20f1240d606",
            "9a833c0b54f04923b2bfe63ef9391e80",
            "e12748661af341f692feb59859d66479",
            "c48ffb3484af402eadfe8deaa53df33a",
            "73101dc90d0c4068898d808cd87ae0d9",
            "3e8a4ff334f0487987df634b76dd0e9c",
            "5df8e93d82cb4f76b23d538ac0e8a381",
            "dc9347c02b114fb3a4678f602d4212f8",
            "416a90dd92bd42b9930d647743a2e6c7",
            "2f9e59fd723f453d874c2d4c138ec991",
            "9836183eb2db4de1b5a50ec8ecaccb70",
            "eb34365af4fe4151b66a6bf16e94ee0d",
            "f46c0525b14a491e98220c1abf6fc686",
            "b0bb80c27d844361bcb63602a33b6765",
            "f94306f08b8e46c1996640b3430407cf",
            "64c9e3dd4b4949d397140379ab5cbe3d",
            "4587762644a54c828f0fe15858f39b68",
            "d2d8f3b636094e0688e2dcbc5555d85a",
            "0f7bd388d2074918875c8a6da943d19b",
            "ad9570536dcf49b6985d01701d949ad4",
            "fc4a3b882e6549f585ee5d029f6c6bbc",
            "dfbd45b0610e416a840ffc83b729ace0",
            "03412b964c4042e68e8a62d2d285a69e",
            "9e2cea286c8c43c0a48cc58718c07d72",
            "b3a679cbcff5433a8e656133f1d90fe5",
            "1330e0e7310d440294ed96c5db0f5cc1",
            "966e9225604248528c91af09380fb2d1",
            "eb4df9ee024443fd882850adae00ce75",
            "919f07d4c807412888ddb2070061fba8",
            "2c895b8b74834cffbbb60c0e5f5bbf67",
            "5905f887622e444c9dd074ea05ebadd1",
            "c4db94279b754355a18c010938ca1f25",
            "73eebb87451d420d966997868d21c1fe",
            "df7ba1aa928d45488fa9ea82f6e01284",
            "138d924b6341435780e73ff02cd03805",
            "c553925063584585af0186508f21e683",
            "3fbb2bea75f0472380f039b237f9d2c0",
            "08d704cfb0d44a7c9ae1a715cfd311e7",
            "bfc55a918829432390fb7f40e83a4f12",
            "e14be090dd374ac39e7fd392940a245f",
            "d24a9a7a789d454981b5fb46cd9a297c",
            "d751281295324efca4ac08fb9b06ca17",
            "29403d8795604a11b139afb1f8274d28",
            "f7da6d24a66f4551803331c055d7fab5",
            "04b1e8e274dd49358f1997e3a1d61580",
            "daf8a5822a904b4eb751f7eb3e6d64cd",
            "f48bac84c3b0415c96e64460467dad15"
          ]
        },
        "outputId": "f041db24-d21e-4d69-d85e-904af034f952"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading corpusâ€¦\n",
            "Loading model: bert-base-uncased\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9b528a13df1a47d5a7b88d190d49d786"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c48ffb3484af402eadfe8deaa53df33a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f94306f08b8e46c1996640b3430407cf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1330e0e7310d440294ed96c5db0f5cc1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3fbb2bea75f0472380f039b237f9d2c0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Target: woke ===\n",
            "Slice 2025-11-26_2025-11-27: 228 occurrences\n",
            "Saved metrics for woke -> outputs_reddit/metrics/woke_metrics.json\n",
            "\n",
            "=== Target: cloud ===\n",
            "Slice 2025-11-26_2025-11-27: 290 occurrences\n",
            "Saved metrics for cloud -> outputs_reddit/metrics/cloud_metrics.json\n",
            "\n",
            "=== Target: nurse ===\n",
            "Slice 2025-11-26_2025-11-27: 149 occurrences\n",
            "Saved metrics for nurse -> outputs_reddit/metrics/nurse_metrics.json\n"
          ]
        }
      ]
    }
  ]
}